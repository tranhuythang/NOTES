{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginalization, Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p 512 - AIAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes network stores cond prob to calculate joint prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a large number of random variables, how to represent the full joint probability (which requires a huge space)?\n",
    "\n",
    "Answer: Bayes network. Bayes network is a graph that represent conditional probabilities; from a Bayes network, one can calculate any joint probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes network is a directed *ACYCLIC* graph (DAG):\n",
    "\n",
    "- Nodes are random variables $X_1,X_2,â€¦$\n",
    "- An edge from $X_1$ to $X_2$ means $X_1$ influences directly $X_2$, i.e. $X_2$ is conditionally depends on $X_1$. If there is no edge between two nodes, the two nodes are independent.\n",
    "***Bayes assumption***: Each variable is conditionally independent of all its nondescendants in the graph given the value of all its parents.\n",
    "$X\\perp Y|parent(X)\\implies P(X|parent(X)Y) = P(X|parent(X)$ or equivalently $P(XY|parent(X)) = P(X|parent(X))P(Y|parent(X))$\n",
    "***Bayes theorem***: $$P(X_1,X_2,...,X_n)=\\prod_{i=1}^{n}P(X_i|parent(X_i))$$\n",
    "This theorem shows how Bayes network encode full join probabilities compactly.\n",
    "\n",
    "Each node $X$ contains a table of conditional probabilities of the form $P(X=x|parent(X)=y)$ which quantifies the influence of $parent(X)$ on $X$\n",
    "\n",
    "So from $X,parent(X)$ the joint probability $P(X)=P(X=x|parent(X)=y)P(parent(X)=y)$ can be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is the posterior $P(parent(X)=y|X=x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formal def (not accurate yet)\n",
    "- a Stochastic process is a family of random variables on a same sample space.\n",
    "- a Markov chain is a sequence of random variables $\\{X_i, i=0,1,2,...\\}$ \n",
    "\n",
    "A Markov chain is a process with a finite number of states in which the probability of being in a particular state at step n + 1 depends only on the state occupied at step n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ProbStat_res/markov.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ProbStat_res/markov2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\vec{p}_n = P^n \\vec{p}_0$\n",
    "\n",
    "We also call the probability vectors $\\vec{p}_0,\\vec{p}_{n},\\vec{p}_{n+1}$ probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability distribution $\\vec{p}$ is called a stationary distribution if $p=Pp$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
